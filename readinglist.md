# Reading List

last updated 10/18



### Understanding Knowledge Distillation

- [x] **[Understanding and Improving Knowledge Distillation](https://arxiv.org/pdf/2002.03532.pdf). Tang et al. Preprint. 2021**
- [ ] [Distilling the knowledge in a neural network](https://arxiv.org/pdf/1503.02531.pdf). Hinton et al. 2015
- [ ] [Born-Again Neural Networks](https://arxiv.org/pdf/1805.04770.pdf). Furlanello et al. ICML'18
- [ ] **[Towards Understanding Knowledge Distillation](https://arxiv.org/pdf/2105.13093.pdf). Phuong et al. ICML'19**
- [ ] **[Self-Distillation Amplifies Regularization in Hilbert Space](https://arxiv.org/pdf/2002.05715.pdf). Mobahi et al. NeurIPS'20**
- [ ] [Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher](https://arxiv.org/pdf/1902.03393.pdf). Mirzadeh et al. AAAI'20
- [ ] [Explaining Knowledge Distillation by Quantifying the Knowledge](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf). Cheng et al. CVPR'20
- [ ] [On the Demystification of Knowledge Distillation: A Residual Network Perspective](https://arxiv.org/pdf/2006.16589.pdf). Jha et al. Preprint. 2020
- [ ] [Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation](https://arxiv.org/pdf/1905.08094.pdf). Zhang et al. ICCV'19
- [ ] **[Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data](https://arxiv.org/pdf/2010.03622.pdf). Wei et al. ICLR'21**
- [ ] [When Does Label Smoothing Help?](https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf). MÃ¼ller et al. NeurIPS'19
- [ ] [Knowledge Distillation as Semiparametric Inference](https://arxiv.org/pdf/2104.09732.pdf). Dao et al. ICLR'21
- [x] **[Understanding Knowledge Distillation in Non-autoregressive Machine Translation](https://openreview.net/pdf?id=BygFVAEKDH). Zhou et al. ICLR'20**



### KD for NLP

- [ ] [Patient Knowledge Distillation for BERT Model Compression](https://arxiv.org/pdf/1908.09355.pdf). Sun et al. EMNLP'19
- [ ] [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/pdf/1909.10351.pdf). Jiao et al. EMNLP'20
- [ ] [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf). Sanh et al. NeurIPS'19
- [ ] [Sequence-Level Knowledge Distillation](https://arxiv.org/pdf/1606.07947.pdf). Kim et al. EMNLP'16
- [ ] TBD



### Pruning for NLP

- [ ] [Movement Pruning: Adaptive Sparsity by Fine-Tuning](https://arxiv.org/pdf/2005.07683.pdf). Sanh et al. NeurIPS'20
- [ ] TBD



### Deep Generative Models for NLP

- [ ] TBD

### Etc.

- [ ] [Harnessing Deep Neural Networks with Logic Rules](https://aclanthology.org/P16-1228.pdf). Hu et al. ACL'16
- [ ] [Lossy Compression for Lossless Prediction](https://arxiv.org/pdf/2106.10800.pdf). Dubois et al. ICLR'21
- [ ] [Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks](https://papers.nips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf). Voelker et al. NeurIPS'19
- [ ] [HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://arxiv.org/pdf/2008.07669.pdf). Gu et al. NeurIPS'20





