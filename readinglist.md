# Reading List

last updated 9/9



### Understanding Knowledge Distillation

- [ ] [Understanding and Improving Knowledge Distillation](https://arxiv.org/pdf/2002.03532.pdf), Tang et al., Preprint
- [ ] Distilling the knowledge in a neural network. Hinton et al.
- [ ] Towards Understanding Knowledge Distillation, Phuong et al., ICML'19
- [ ] [Self-Distillation Amplifies Regularization in Hilbert Space](https://arxiv.org/pdf/2002.05715.pdf), Mobahi et al., NeurIPS'20
- [ ] [Explaining Knowledge Distillation by Quantifying the Knowledge](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf), Cheng et al., CVPR'20
- [ ] On the Demystification of Knowledge Distillation: A Residual Network Perspective
- [ ] Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation
- [ ] Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data
- [ ] EVEN YOUR TEACHER NEEDS GUIDANCE: GROUND-TRUTH TARGETS DAMPEN REGULARIZATION IMPOSED BY SELF-DISTILLATION
- [ ] When Does Label Smoothing Help?



KD for NLP

1. Patient Knowledge Distillation for BERT Model Compression. Sun, Siqi et al. arXiv:1908.09355
2. TinyBERT: Distilling BERT for Natural Language Understanding. Jiao, Xiaoqi et al. arXiv:1909.10351
3. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Sanh, Victor et al. arXiv:1910.01108
4. Sequence-Level Knowledge Distillation, Kim et al., 



Pruning

Movement Pruning: Adaptive Sparsity by Fine-Tuning, Sanh et al., 

